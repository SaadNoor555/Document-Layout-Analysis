{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook was inspired from [here](https://www.kaggle.com/code/ammarnassanalhajali/layout-parser-model-training)","metadata":{"_uuid":"2a9e103a-9be5-4118-ace4-e429b1c32450","_cell_guid":"b8a4d973-2613-48d4-83fd-2e9d65cc5e91","trusted":true}},{"cell_type":"markdown","source":"# Detectron2\n\n[Detectron2](https://detectron2.readthedocs.io/en/latest/index.html) is a popular open-source software library developed by Facebook AI Research (FAIR) for building computer vision models. It serves as a powerful framework for object detection, instance segmentation, and keypoint detection tasks. Detectron2 is built on top of PyTorch, geared towards a more convenient way to build modular, flexible pipelines for specific Computer Vision Tasks such as object detection, instance segmentation. \n\nDetectron2 has a collection of trained models for these tasks in their [model zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md). We can also use detectron2 to train pre-implemented state-of-the-art models from scratch for new datasets, as we do in this notebook. \n\nRead the [documentation](https://detectron2.readthedocs.io/en/latest/index.html).","metadata":{"_uuid":"02db164f-14fe-473f-9360-3528dc711121","_cell_guid":"acaa7248-40b8-4c14-9e01-ccea0d785f9b","trusted":true}},{"cell_type":"markdown","source":"# 1 Install detectron2","metadata":{"_uuid":"dcbab65a-2deb-44b8-9110-956e73ae7f31","_cell_guid":"7f55d342-6fbb-4797-82e6-b679e324be3c","trusted":true}},{"cell_type":"markdown","source":"## 1.1 Recommended Way (is not working on kaggle)","metadata":{"_uuid":"38f68376-6839-49d5-8be7-f0bf92f9e747","_cell_guid":"bb56b025-c2e8-4555-8e32-52d13c05128e","trusted":true}},{"cell_type":"code","source":"# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'","metadata":{"_uuid":"7d1fb578-8ef5-4e24-bd21-c65ecb29d5de","_cell_guid":"be83a0e9-a75a-476d-b7fa-c93d19c3f838","collapsed":false,"_kg_hide-input":false,"_kg_hide-output":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"","metadata":{"_uuid":"ccf71dcd-74a9-4f25-81c6-3ab00698c859","_cell_guid":"eb606811-bf2e-45e4-a167-08a90af1cab4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:25:21.410992Z","iopub.execute_input":"2023-08-05T01:25:21.411315Z","iopub.status.idle":"2023-08-05T01:25:21.421950Z","shell.execute_reply.started":"2023-08-05T01:25:21.411288Z","shell.execute_reply":"2023-08-05T01:25:21.421062Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Fast Way\nIgnore the warnings.","metadata":{"_uuid":"ea41b1ca-563d-482f-871c-ecb44dfd1411","_cell_guid":"9cb7c8f7-2854-475b-a217-53d75602d543","trusted":true}},{"cell_type":"code","source":"%%capture\nimport sys, os, distutils.core\n# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n!git clone 'https://github.com/facebookresearch/detectron2'\ndist = distutils.core.run_setup(\"./detectron2/setup.py\")\n!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\nsys.path.insert(0, os.path.abspath('./detectron2'))","metadata":{"_uuid":"6cbb4532-5e4a-4472-93ee-668b7b59173e","_cell_guid":"fcc9bb73-4ea3-4a1f-9bfd-60d6238ee3cf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:25:21.430161Z","iopub.execute_input":"2023-08-05T01:25:21.430482Z","iopub.status.idle":"2023-08-05T01:26:11.748391Z","shell.execute_reply.started":"2023-08-05T01:25:21.430458Z","shell.execute_reply":"2023-08-05T01:26:11.746998Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 2 Notebook Config","metadata":{"_uuid":"a4259cfb-08a6-4cc2-8f3f-c88307bc957e","_cell_guid":"3773d64b-e163-44f4-a67b-d1aa735ea789","trusted":true}},{"cell_type":"code","source":"\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n\n\nfrom datetime import datetime\n\n# if False, model is set to `PRETRAINED_PATH` model\nis_train = True\n\n# if True, evaluate on validation dataset\nis_evaluate = True\n\n# if True, run inference on test dataset\nis_inference = True\n\n# if True and `is_train` == True, `PRETRAINED_PATH` model is trained further\nis_resume_training = False\n\n# Perform augmentation\nis_augment = False\n\nSEED = 42\nimport random\nimport os\nimport numpy as np\nimport torch\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\n\"\"\"## 2.2 Paths\"\"\"\n\nfrom pathlib import Path\n\nTRAIN_IMG_DIR = Path(\"/kaggle/input/binarizedbadlad/binTrain\")\n\nTRAIN_COCO_PATH = Path(\"/kaggle/input/dlsprint2/badlad/labels/coco_format/train/badlad-train-coco.json\")\n\nTEST_IMG_DIR = Path(\"/kaggle/input/dlsprint2/badlad/images/test\")\n\nTEST_METADATA_PATH = Path(\"/kaggle/input/dlsprint2/badlad/badlad-test-metadata.json\")\n\n# Training output directory\nOUTPUT_DIR = Path(\"./output\")\nOUTPUT_MODEL = OUTPUT_DIR/\"model_final.pth\"\n\n# Path to your pretrained model weights\nPRETRAINED_PATH = Path(\"\")\n\n\"\"\"## 2.3 imports\"\"\"\n\n# detectron2\nfrom detectron2.utils.memory import retry_if_cuda_oom\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.modeling import build_model\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nimport detectron2.data.transforms as T\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader, DatasetMapper\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.structures import BoxMode\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2 import model_zoo\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm  # progress bar\nimport matplotlib.pyplot as plt\nimport json\nimport cv2\nimport copy\nfrom typing import Optional\n\nfrom IPython.display import FileLink\n\n# torch\nimport torch\nimport os\n\nimport gc\n\nimport warnings\n# Ignore \"future\" warnings and Data-Frame-Slicing warnings.\nwarnings.filterwarnings('ignore')\n\nsetup_logger()\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n\n\"\"\"# 3 COCO Annotations Data\n\n## 3.1 Load\n\"\"\"\n\nwith TEST_METADATA_PATH.open() as f:\n    test_dict = json.load(f)\n\n\nprint(\"#### LABELS AND METADATA LOADED ####\")\n\n\"\"\"## 3.2 Observe\"\"\"\n\ndef organize_coco_data(data_dict: dict):\n    thing_classes: list[str] = []\n\n    # Map Category Names to IDs\n    for cat in data_dict['categories']:\n        thing_classes.append(cat['name'])\n\n    print(thing_classes)\n\n    # thing_classes = ['paragraph', 'text_box', 'image', 'table']\n    # Images\n    images_metadata: list[dict] = data_dict['images']\n\n    # Convert COCO annotations to detectron2 annotations format\n    data_annotations = []\n    for ann in data_dict['annotations']:\n        # coco format -> detectron2 format\n        annot_obj = {\n            # Annotation ID\n            \"id\": ann['id'],\n\n            # Segmentation Polygon (x, y) coords\n            \"gt_masks\": ann['segmentation'],\n\n            # Image ID for this annotation (Which image does this annotation belong to?)\n            \"image_id\": ann['image_id'],\n\n            # Category Label (0: paragraph, 1: text box, 2: image, 3: table)\n            \"category_id\": ann['category_id'],\n\n            \"x_min\": ann['bbox'][0],  # left\n            \"y_min\": ann['bbox'][1],  # top\n            \"x_max\": ann['bbox'][0] + ann['bbox'][2],  # left+width\n            \"y_max\": ann['bbox'][1] + ann['bbox'][3]  # top+height\n        }\n        data_annotations.append(annot_obj)\n\n    return thing_classes, images_metadata, data_annotations\n\n\nthing_classes_test, images_metadata_test, _ = organize_coco_data(\n    test_dict\n)\n\nthing_classes = thing_classes_test\nprint(\"THINGS CLASSES\")\nprint(thing_classes)\ntest_metadata = pd.DataFrame(images_metadata_test)\ntest_metadata = test_metadata[['id', 'file_name', 'width', 'height']]\ntest_metadata = test_metadata.rename(columns={\"id\": \"image_id\"})\nprint(\"test_metadata size=\", len(test_metadata))\ntest_metadata.head(5)\n\ndef convert_coco_to_detectron2_format(\n    imgdir: Path,\n    metadata_df: pd.DataFrame,\n    annot_df: Optional[pd.DataFrame] = None,\n    target_indices: Optional[np.ndarray] = None,\n):\n\n    dataset_dicts = []\n    for _, train_meta_row in tqdm(metadata_df.iterrows(), total=len(metadata_df)):\n        # Iterate over each image\n        image_id, filename, width, height = train_meta_row.values\n\n        annotations = []\n\n        # If train/validation data, then there will be annotations\n        if annot_df is not None:\n            for _, ann in annot_df.query(\"image_id == @image_id\").iterrows():\n                # Get annotations of current iteration's image\n                class_id = ann[\"category_id\"]\n                gt_masks = ann[\"gt_masks\"]\n                bbox_resized = [\n                    float(ann[\"x_min\"]),\n                    float(ann[\"y_min\"]),\n                    float(ann[\"x_max\"]),\n                    float(ann[\"y_max\"]),\n                ]\n\n                annotation = {\n                    \"bbox\": bbox_resized,\n                    \"bbox_mode\": BoxMode.XYXY_ABS,\n                    \"segmentation\": gt_masks,\n                    \"category_id\": class_id,\n                }\n\n                annotations.append(annotation)\n\n        # coco format -> detectron2 format dict\n        record = {\n            \"file_name\": str(imgdir/filename),\n            \"image_id\": image_id,\n            \"width\": width,\n            \"height\": height,\n            \"annotations\": annotations\n        }\n\n        dataset_dicts.append(record)\n\n    if target_indices is not None:\n        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n\n    return dataset_dicts\n\n\n\"\"\"## 4.3 Registering and Loading Data for `detectron2`\"\"\"\nDATA_REGISTER_TEST     = \"badlad_test\"\n\n# Register Test Inference data\nDatasetCatalog.register(\n    DATA_REGISTER_TEST,\n    lambda: convert_coco_to_detectron2_format(\n        TEST_IMG_DIR,\n        test_metadata,\n    )\n)\n\n# Set Test data categories\nMetadataCatalog.get(DATA_REGISTER_TEST).set(\n    thing_classes=thing_classes_test\n)\n\n# dataset_dicts_test = DatasetCatalog.get(DATA_REGISTER_TEST)\nmetadata_dicts_test = MetadataCatalog.get(DATA_REGISTER_TEST)","metadata":{"_uuid":"a2b86369-4dc4-4c48-8a0a-24f1dbed9b4c","_cell_guid":"f5920213-4dd4-4d65-8d7d-ee0a7b3d9343","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:11.752171Z","iopub.execute_input":"2023-08-05T01:26:11.753571Z","iopub.status.idle":"2023-08-05T01:26:12.834420Z","shell.execute_reply.started":"2023-08-05T01:26:11.753529Z","shell.execute_reply":"2023-08-05T01:26:12.833416Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"#### LABELS AND METADATA LOADED ####\n['paragraph', 'text_box', 'image', 'table']\nTHINGS CLASSES\n['paragraph', 'text_box', 'image', 'table']\ntest_metadata size= 13000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"0348b282-e449-4dc4-8bda-5aff605352c5","_cell_guid":"c2b93beb-b981-471f-8976-19d051462ad8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! git clone https://github.com/microsoft/unilm.git --depth=1 --quiet\n! sed -i 's/from collections import Iterable/from collections.abc import Iterable/' unilm/dit/object_detection/ditod/table_evaluation/data_structure.py","metadata":{"_uuid":"bbfa1340-e1f6-4b4a-9448-c7082cdbfa96","_cell_guid":"6f0c5632-81a2-4ee1-836c-f23fd97c13ef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:12.836101Z","iopub.execute_input":"2023-08-05T01:26:12.836466Z","iopub.status.idle":"2023-08-05T01:26:18.767136Z","shell.execute_reply.started":"2023-08-05T01:26:12.836433Z","shell.execute_reply":"2023-08-05T01:26:18.765776Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append(\"unilm\")\n\nimport cv2\n\nfrom unilm.dit.object_detection.ditod import add_vit_config","metadata":{"_uuid":"e615baeb-945f-4063-a93c-963a5be33a72","_cell_guid":"da69737a-2f69-465c-a681-6066e6be6ba2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:18.771365Z","iopub.execute_input":"2023-08-05T01:26:18.772194Z","iopub.status.idle":"2023-08-05T01:26:20.150643Z","shell.execute_reply.started":"2023-08-05T01:26:18.772164Z","shell.execute_reply":"2023-08-05T01:26:20.149682Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%%writefile cascade_dit_base.yaml\n_BASE_: \"/kaggle/input/dit-publay-finetuned/Base-RCNN-FPN.yaml\"\nMODEL:\n  PIXEL_MEAN: [ 127.5, 127.5, 127.5 ]\n  PIXEL_STD: [ 127.5, 127.5, 127.5 ]\n  WEIGHTS: \"https://layoutlm.blob.core.windows.net/dit/dit-pts/dit-base-224-p16-500k-62d53a.pth\"\n  VIT:\n    NAME: \"dit_base_patch16\"\n  ROI_HEADS:\n    NAME: CascadeROIHeads\n  ROI_BOX_HEAD:\n    CLS_AGNOSTIC_BBOX_REG: True\n  RPN:\n    POST_NMS_TOPK_TRAIN: 2000\nSOLVER:\n  WARMUP_ITERS: 1000\n  IMS_PER_BATCH: 16\n  MAX_ITER: 60000\n  CHECKPOINT_PERIOD: 2000\nTEST:\n  EVAL_PERIOD: 2000","metadata":{"_uuid":"19abaefc-d466-49e4-b9ca-8be2e5873d65","_cell_guid":"d364d0a9-de62-4227-a2fb-a0a346ad49a6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:20.151922Z","iopub.execute_input":"2023-08-05T01:26:20.152267Z","iopub.status.idle":"2023-08-05T01:26:20.161734Z","shell.execute_reply.started":"2023-08-05T01:26:20.152235Z","shell.execute_reply":"2023-08-05T01:26:20.159937Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing cascade_dit_base.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"_uuid":"e3fce8d5-dd99-460f-936e-7c5ad6fee623","_cell_guid":"50a2d031-2d3d-4ca0-bf92-4ccc174d3f19","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:20.163209Z","iopub.execute_input":"2023-08-05T01:26:20.163575Z","iopub.status.idle":"2023-08-05T01:26:20.389195Z","shell.execute_reply.started":"2023-08-05T01:26:20.163542Z","shell.execute_reply":"2023-08-05T01:26:20.388239Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"76"},"metadata":{}}]},{"cell_type":"code","source":"def rebuild_model():\n    model = build_model(inf_cfg)\n    _ = DetectionCheckpointer(model).load(inf_cfg.MODEL.WEIGHTS)\n    return model","metadata":{"_uuid":"44c4ad7d-f5c6-4eb4-8493-16fbc977d4aa","_cell_guid":"b4799c1b-a888-4f8d-80fb-1cccd59d0377","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:20.390808Z","iopub.execute_input":"2023-08-05T01:26:20.391474Z","iopub.status.idle":"2023-08-05T01:26:20.400818Z","shell.execute_reply.started":"2023-08-05T01:26:20.391441Z","shell.execute_reply":"2023-08-05T01:26:20.399759Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"inf_cfg = get_cfg()\nadd_vit_config(inf_cfg)\ninf_cfg.merge_from_file(\"/kaggle/working/cascade_dit_base.yaml\")\ninf_cfg.SOLVER.IMS_PER_BATCH = 64\ninf_cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\ninf_cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\ninf_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25\ninf_cfg.MODEL.DEVICE = \"cuda\"\ninf_cfg.DATALOADER.NUM_WORKERS = 2  # lower this if CUDA overflow occurs\ninf_cfg.MODEL.WEIGHTS = str(\"/kaggle/input/dit-publay-finetuned/dit-pub-50000.pth\")\ninf_cfg.OUTPUT_DIR = str(OUTPUT_DIR)\nprint(\"creating cfg.OUTPUT_DIR -> \", inf_cfg.OUTPUT_DIR)\nOUTPUT_DIR.mkdir(exist_ok=True)\nmodel = rebuild_model()\nmodel.eval()","metadata":{"_uuid":"5d28c6a4-a958-4594-bd87-655b95b6f11a","_cell_guid":"b15ce3c2-639c-4bb7-ad55-144f10baa645","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:20.401959Z","iopub.execute_input":"2023-08-05T01:26:20.402261Z","iopub.status.idle":"2023-08-05T01:26:30.457893Z","shell.execute_reply.started":"2023-08-05T01:26:20.402231Z","shell.execute_reply":"2023-08-05T01:26:30.456967Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"creating cfg.OUTPUT_DIR ->  output\n\u001b[32m[08/05 01:26:25 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /kaggle/input/dit-publay-finetuned/dit-pub-50000.pth ...\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"GeneralizedRCNN(\n  (backbone): FPN(\n    (fpn_lateral2): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral3): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral4): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (fpn_lateral5): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (top_block): LastLevelMaxPool()\n    (bottom_up): VIT_Backbone(\n      (backbone): BEiT(\n        (patch_embed): PatchEmbed(\n          (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (pos_drop): Dropout(p=0.0, inplace=False)\n        (blocks): ModuleList(\n          (0): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (1): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.00909090880304575)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (2): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.0181818176060915)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (3): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.027272727340459824)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (4): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.036363635212183)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (5): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.045454543083906174)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (6): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.054545458406209946)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (7): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.06363636255264282)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (8): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.0727272778749466)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (9): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.08181818574666977)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (10): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.09090909361839294)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (11): Block(\n            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=768, out_features=768, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): DropPath(p=0.10000000149011612)\n            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (fpn1): Sequential(\n          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n          (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): GELU(approximate='none')\n          (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n        )\n        (fpn2): Sequential(\n          (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))\n        )\n        (fpn3): Identity()\n        (fpn4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      )\n    )\n  )\n  (proposal_generator): RPN(\n    (rpn_head): StandardRPNHead(\n      (conv): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (anchor_generator): DefaultAnchorGenerator(\n      (cell_anchors): BufferList()\n    )\n  )\n  (roi_heads): CascadeROIHeads(\n    (box_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (box_head): ModuleList(\n      (0-2): 3 x FastRCNNConvFCHead(\n        (flatten): Flatten(start_dim=1, end_dim=-1)\n        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n        (fc_relu1): ReLU()\n        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n        (fc_relu2): ReLU()\n      )\n    )\n    (box_predictor): ModuleList(\n      (0-2): 3 x FastRCNNOutputLayers(\n        (cls_score): Linear(in_features=1024, out_features=5, bias=True)\n        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n      )\n    )\n    (mask_pooler): ROIPooler(\n      (level_poolers): ModuleList(\n        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n      )\n    )\n    (mask_head): MaskRCNNConvUpsampleHead(\n      (mask_fcn1): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn2): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn3): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (mask_fcn4): Conv2d(\n        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n        (activation): ReLU()\n      )\n      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n      (deconv_relu): ReLU()\n      (predictor): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def binarize(image):\n    # Convert image to grayscale if it has more than one channel\n    if len(image.shape) > 2 and image.shape[2] in [3, 4]:\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Binarize the grayscale image\n    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n\n    # Convert the binary image back to 3-channel format (optional, but could be useful for consistency)\n    binary_image = cv2.cvtColor(binary_image, cv2.COLOR_GRAY2RGB)\n\n    return binary_image\n","metadata":{"_uuid":"535a6cad-011c-4809-bc82-dc25b01193f7","_cell_guid":"48f12f79-977e-4184-93f3-afe2ce2ea1a5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:57:12.764578Z","iopub.execute_input":"2023-08-05T01:57:12.765012Z","iopub.status.idle":"2023-08-05T01:57:12.773244Z","shell.execute_reply.started":"2023-08-05T01:57:12.764977Z","shell.execute_reply":"2023-08-05T01:57:12.772117Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"class BinarizeDatasetMapper(DatasetMapper):\n    def __init__(self, cfg, is_train=True):\n        super().__init__(cfg, is_train)\n\n    def __call__(self, dataset_dict):\n        dataset_dict = super().__call__(dataset_dict)\n        image = dataset_dict[\"image\"].permute(1, 2, 0).cpu().numpy()\n        binarized_image = binarize(image)\n        dataset_dict[\"image\"] = torch.tensor(binarized_image.transpose(2, 0, 1)).to(dataset_dict[\"image\"].device)\n        return dataset_dict","metadata":{"_uuid":"3ba6e993-cd55-47b4-bc92-91c055b453de","_cell_guid":"47a1e74a-1092-414b-9b19-2f03f48cad5d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:26:30.469380Z","iopub.execute_input":"2023-08-05T01:26:30.469692Z","iopub.status.idle":"2023-08-05T01:26:30.480466Z","shell.execute_reply.started":"2023-08-05T01:26:30.469667Z","shell.execute_reply":"2023-08-05T01:26:30.479441Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    images = [data[\"image\"].cpu().numpy() for data in batch]\n    images = [binarize(image) for image in images]\n    images = torch.stack([torch.tensor(image.transpose(2, 0, 1)) for image in images])\n    batch[0][\"image\"] = images\n    return batch\n","metadata":{"execution":{"iopub.status.busy":"2023-08-05T01:57:17.442269Z","iopub.execute_input":"2023-08-05T01:57:17.442629Z","iopub.status.idle":"2023-08-05T01:57:17.448837Z","shell.execute_reply.started":"2023-08-05T01:57:17.442601Z","shell.execute_reply":"2023-08-05T01:57:17.447932Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"BATCH = 2  # lower this if CUDA overflow occurs\n# mapper = BinarizeDatasetMapper(inf_cfg, is_train=False)\ntest_loader = build_detection_test_loader(inf_cfg, DATA_REGISTER_TEST,batch_size=BATCH)","metadata":{"_uuid":"b9b60d2d-3772-4857-8b74-83e0330aef2a","_cell_guid":"b74cfd88-33f9-48e9-884e-cf70be8f869c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T02:00:11.431955Z","iopub.execute_input":"2023-08-05T02:00:11.432376Z","iopub.status.idle":"2023-08-05T02:00:12.623169Z","shell.execute_reply.started":"2023-08-05T02:00:11.432340Z","shell.execute_reply":"2023-08-05T02:00:12.622265Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e203c1e5628b4d9d92a2a254ca0fedbc"}},"metadata":{}},{"name":"stdout","text":"\u001b[32m[08/05 02:00:12 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n\u001b[32m[08/05 02:00:12 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n\u001b[32m[08/05 02:00:12 d2.data.common]: \u001b[0mSerializing 13000 elements to byte tensors and concatenating them all ...\n\u001b[32m[08/05 02:00:12 d2.data.common]: \u001b[0mSerialized dataset takes 2.07 MiB\n","output_type":"stream"}]},{"cell_type":"code","source":"# ACCEPTANCE_THRESHOLD = 0.6","metadata":{"_uuid":"3a04ea15-7fd9-4efd-a66a-3101601de529","_cell_guid":"9de4d55c-c58d-45ed-b069-be8c81164f8d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:53:44.632439Z","iopub.execute_input":"2023-08-05T01:53:44.632782Z","iopub.status.idle":"2023-08-05T01:53:44.637378Z","shell.execute_reply.started":"2023-08-05T01:53:44.632753Z","shell.execute_reply":"2023-08-05T01:53:44.636480Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-05T01:54:03.706593Z","iopub.execute_input":"2023-08-05T01:54:03.707318Z","iopub.status.idle":"2023-08-05T01:54:03.961914Z","shell.execute_reply.started":"2023-08-05T01:54:03.707282Z","shell.execute_reply":"2023-08-05T01:54:03.959933Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"def rle_encode(mask):\n    pixels = mask.T.flatten()\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return ' '.join(str(x) for x in rle)","metadata":{"_uuid":"da2c44bc-1e96-45d2-923e-c27bad9995e2","_cell_guid":"302679e7-c6e3-4221-a127-e91f8b9c2f8a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:51:04.679454Z","iopub.execute_input":"2023-08-05T01:51:04.679807Z","iopub.status.idle":"2023-08-05T01:51:04.687233Z","shell.execute_reply.started":"2023-08-05T01:51:04.679778Z","shell.execute_reply":"2023-08-05T01:51:04.686312Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"ACCEPTANCE_THRESHOLDS = {\n    \"paragraph\": 0.5,\n    \"text_box\": 0.3,\n    \"image\": 0.5,\n    \"table\": 0.55,\n}\n\n# @retry_if_cuda_oom\n# def get_masks(prediction):\n#     # get masks for each category\n#     pred_masks = (prediction.pred_masks != 0)\n#     pred_classes = prediction.pred_classes\n\n#     rles = []\n#     for cat in range(len(thing_classes_test)):\n#         pred_mask = pred_masks[pred_classes == cat]\n#         pred_mask = torch.any(pred_mask, dim=0)\n        \n#         threshold = ACCEPTANCE_THRESHOLDS[thing_classes[cat]]\n#         take = prediction.scores >= threshold\n#         pred_mask = pred_mask & take\n        \n#         rles.append(rle_encode(pred_mask.short().to(\"cpu\").numpy()))\n\n#     return rles\n\ndef get_masks(prediction):\n    # get masks for each category\n    rles = []\n    for cat in range(len(thing_classes)):\n        threshold = ACCEPTANCE_THRESHOLDS.get(thing_classes[cat], 0.4)  # Get threshold or set to 0.4 if not present\n        if threshold==0.4:\n            print(\"thresh : 0.4\")\n        take = prediction.scores >= threshold\n        pred_masks = (prediction.pred_masks[take] != 0)\n        pred_classes = prediction.pred_classes[take]\n        \n        pred_mask = torch.any(pred_masks[pred_classes == cat], dim=0)\n        rles.append(rle_encode(pred_mask.short().to(\"cpu\").numpy()))\n\n    return rles\n\n\n# def get_masks(prediction):\n#     # get masks for each category\n#     take = prediction.scores >= ACCEPTANCE_THRESHOLD\n#     pred_masks = (prediction.pred_masks[take] != 0)\n#     pred_classes = prediction.pred_classes[take]\n  \n#     rles = []\n#     for cat in range(len(thing_classes)):\n#         pred_mask = pred_masks[pred_classes == cat]\n        \n#         # pred_mask = retry_if_cuda_oom(torch.any)(pred_mask, dim=0)\n#         pred_mask = torch.any(pred_mask, dim=0)\n#         rles.append(rle_encode(pred_mask.short().to(\"cpu\").numpy()))\n\n#     return rles","metadata":{"_uuid":"32fe4ae1-f0a5-4ef5-8318-4c54c04ebc7e","_cell_guid":"f0521c26-8932-4e03-87c6-3f128065a1ef","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:51:05.063635Z","iopub.execute_input":"2023-08-05T01:51:05.064397Z","iopub.status.idle":"2023-08-05T01:51:05.074488Z","shell.execute_reply.started":"2023-08-05T01:51:05.064355Z","shell.execute_reply":"2023-08-05T01:51:05.073569Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def run_inference(data):\n    results = []\n    with torch.no_grad():\n        outputs = model(data)\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n\n        for idx, output in enumerate(outputs):\n            output = output[\"instances\"]\n\n            rles = get_masks(output)\n\n            result = [\n                f\"{data[idx]['image_id']}_{cat},{rles[cat]}\\n\"\n                for cat in range(len(thing_classes))\n            ]\n\n            results.extend(result)\n\n        del outputs, output\n\n    return results","metadata":{"_uuid":"0a716fc2-7c5d-45a3-a30a-735bb994cf8c","_cell_guid":"dea5f5f6-e9b9-46a0-aa5f-2da6b6518a59","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:51:05.310216Z","iopub.execute_input":"2023-08-05T01:51:05.310880Z","iopub.status.idle":"2023-08-05T01:51:05.319790Z","shell.execute_reply.started":"2023-08-05T01:51:05.310827Z","shell.execute_reply":"2023-08-05T01:51:05.318792Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"submission_file = open(\"submission.csv\", \"w\")\nsubmission_file.write(\"Id,Predicted\\n\")\n\nresults: list[str] = []\n\nfor i, data in enumerate(tqdm(test_loader)):\n    res = run_inference(data)\n    results.extend(res)\n\n    if i % (500 // BATCH) == 0:\n        print(f\"Inference on batch {i}/{len(test_loader)} done\")\n        submission_file.writelines(results)\n        results = []\n\nsubmission_file.writelines(results)\nsubmission_file.close()","metadata":{"_uuid":"aa945c38-e2bd-4cc8-a2af-9fb270ac9c9b","_cell_guid":"d3a579d0-d2b3-4acf-9eb6-5a9ac199ee3b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T02:00:43.326780Z","iopub.execute_input":"2023-08-05T02:00:43.327510Z","iopub.status.idle":"2023-08-05T02:01:02.738842Z","shell.execute_reply.started":"2023-08-05T02:00:43.327477Z","shell.execute_reply":"2023-08-05T02:01:02.737066Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd3dda4bdab4059a85a7692fe98b752"}},"metadata":{}},{"name":"stdout","text":"Inference on batch 0/6500 done\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m results: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(test_loader)):\n\u001b[0;32m----> 7\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(res)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m500\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BATCH) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[32], line 4\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      6\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/meta_arch/rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/meta_arch/rcnn.py:208\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m         proposals, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/proposal_generator/rpn.py:452\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    images (ImageList): input images of length `N`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    loss: dict[Tensor] or None\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[0;32m--> 452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn_head(features)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/anchor_generator.py:230\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    features (list[Tensor]): list of backbone feature maps on which to generate anchors.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        where Hi, Wi are resolution of the feature map divided by anchor stride.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m grid_sizes \u001b[38;5;241m=\u001b[39m [feature_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 230\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grid_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Boxes(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m anchors_over_all_feature_maps]\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/anchor_generator.py:174\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator._grid_anchors\u001b[0;34m(self, grid_sizes)\u001b[0m\n\u001b[1;32m    172\u001b[0m buffers: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_anchors\u001b[38;5;241m.\u001b[39mnamed_buffers()]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, stride, base_anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grid_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, buffers):\n\u001b[0;32m--> 174\u001b[0m     shift_x, shift_y \u001b[38;5;241m=\u001b[39m \u001b[43m_create_grid_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_anchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     shifts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((shift_x, shift_y, shift_x, shift_y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    177\u001b[0m     anchors\u001b[38;5;241m.\u001b[39mappend((shifts\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m base_anchors\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/modeling/anchor_generator.py:43\u001b[0m, in \u001b[0;36m_create_grid_offsets\u001b[0;34m(size, stride, offset, target_device_tensor)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_grid_offsets\u001b[39m(\n\u001b[1;32m     40\u001b[0m     size: List[\u001b[38;5;28mint\u001b[39m], stride: \u001b[38;5;28mint\u001b[39m, offset: \u001b[38;5;28mfloat\u001b[39m, target_device_tensor: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     grid_height, grid_width \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m---> 43\u001b[0m     shifts_x \u001b[38;5;241m=\u001b[39m \u001b[43mmove_device_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     shifts_y \u001b[38;5;241m=\u001b[39m move_device_like(\n\u001b[1;32m     48\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(offset \u001b[38;5;241m*\u001b[39m stride, grid_height \u001b[38;5;241m*\u001b[39m stride, step\u001b[38;5;241m=\u001b[39mstride, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     49\u001b[0m         target_device_tensor,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     shift_y, shift_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(shifts_y, shifts_x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/jit/_trace.py:1220\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   1219\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> 1220\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1222\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/kaggle/working/detectron2/detectron2/layers/wrappers.py:162\u001b[0m, in \u001b[0;36mmove_device_like\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_if_tracing\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_device_like\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, dst: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Tracing friendly way to cast tensor to another tensor's device. Device will be treated\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    as constant during tracing, scripting the casting process as whole can workaround this issue.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"if Path(\"submission.csv\").exists:\n    display(FileLink(\"submission.csv\"))","metadata":{"_uuid":"517a1805-46b6-4fb6-9372-31f7661c97be","_cell_guid":"1185d525-562c-475d-b98b-f58c96d82ea2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:51:06.124088Z","iopub.status.idle":"2023-08-05T01:51:06.124910Z","shell.execute_reply.started":"2023-08-05T01:51:06.124636Z","shell.execute_reply":"2023-08-05T01:51:06.124675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -r detectron2/","metadata":{"_uuid":"26cdb869-5bee-404f-8581-0f6899a22223","_cell_guid":"0ed19be4-de35-495b-8ffd-aa4c950752f6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-05T01:29:45.318776Z","iopub.status.idle":"2023-08-05T01:29:45.320216Z","shell.execute_reply.started":"2023-08-05T01:29:45.319975Z","shell.execute_reply":"2023-08-05T01:29:45.319997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}